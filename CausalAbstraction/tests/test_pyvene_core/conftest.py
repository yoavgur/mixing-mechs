# tests/test_experiments/conftest.py
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).resolve().parent.parent.parent))

import pytest
import torch
import random
import numpy as np
from collections import defaultdict
from unittest.mock import MagicMock, patch

from causal.causal_model import CausalModel
from causal.counterfactual_dataset import CounterfactualDataset
from neural.pipeline import LMPipeline
from neural.LM_units import TokenPosition, ResidualStream


@pytest.fixture(scope="session")
def mcqa_causal_model():
    """Create a simple MCQA causal model fixture."""
    # Define model variables
    NUM_CHOICES = 4
    ALPHABET = "ABCDEFGHIJKLMNOPQRSTUVWXYZ"
    
    # Define object/color pairs for the questions
    COLOR_OBJECTS = [
        ("red", "apple"), ("yellow", "banana"), ("green", "leaf"),
        ("blue", "sky"), ("brown", "chocolate"), ("white", "snow"),
        ("black", "coal"), ("purple", "grape"), ("orange", "carrot")
    ]
    
    COLORS = [item[0] for item in COLOR_OBJECTS]
    
    # FIXED: Include raw_input and raw_output as required by the new CausalModel
    variables = ["raw_input", "question"] + [f"symbol{x}" for x in range(NUM_CHOICES)] + \
                [f"choice{x}" for x in range(NUM_CHOICES)] + ["answer_pointer", "answer", "raw_output"]
    
    values = {f"choice{x}": COLORS for x in range(NUM_CHOICES)}
    values.update({f"symbol{x}": list(ALPHABET) for x in range(NUM_CHOICES)})
    values.update({"answer_pointer": list(range(NUM_CHOICES)), "answer": list(ALPHABET)})
    values.update({"question": COLOR_OBJECTS})
    # FIXED: Change raw_input to have a list of possible values instead of None
    # Since raw_input is generated by mechanisms, we can use a dummy list
    values.update({"raw_input": [""], "raw_output": [""]})
    
    # FIXED: Update parent relationships to include raw_input and raw_output
    parents = {"answer": ["answer_pointer"] + [f"symbol{x}" for x in range(NUM_CHOICES)], 
               "answer_pointer": ["question"] + [f"choice{x}" for x in range(NUM_CHOICES)],
               "raw_output": ["answer"],  # raw_output depends on answer
               "raw_input": [],  # raw_input is an input variable
               "question": []}
    parents.update({f"choice{x}": [] for x in range(NUM_CHOICES)})
    parents.update({f"symbol{x}": [] for x in range(NUM_CHOICES)})
    
    # Define causal mechanisms
    def get_raw_input():
        # Return empty string for raw_input (will be filled by input_dumper)
        return ""
    
    def get_question():
        return random.choice(COLOR_OBJECTS)
    
    def get_symbol():
        return random.choice(list(ALPHABET))
    
    def get_choice():
        return random.choice(COLORS)
    
    def get_answer_pointer(question, *choices):
        for i, choice in enumerate(choices):
            if choice == question[0]:  # question[0] is the color
                return i
        return random.randint(0, NUM_CHOICES - 1)
    
    def get_answer(answer_pointer, *symbols):
        return " " + symbols[answer_pointer]
    
    # FIXED: Add raw_output mechanism
    def get_raw_output(answer):
        return answer
    
    mechanisms = {
        "raw_input": get_raw_input,  # FIXED: Add raw_input mechanism
        "question": get_question,
        **{f"symbol{i}": get_symbol for i in range(NUM_CHOICES)},
        **{f"choice{i}": get_choice for i in range(NUM_CHOICES)},
        "answer_pointer": get_answer_pointer,
        "answer": get_answer,
        "raw_output": get_raw_output  # FIXED: Add raw_output mechanism
    }
    
    # Create and return the model
    return CausalModel(
        variables, values, parents, mechanisms, 
        id="4_answer_MCQA_test"
    )

@pytest.fixture(scope="session")
def mcqa_counterfactual_datasets(mcqa_causal_model):
    """Generate test counterfactual datasets for the MCQA task."""
    model = mcqa_causal_model
    NUM_CHOICES = 4
    
    def is_input_valid(x):
        question_color = x["question"][0]
        choice_colors = [x[f"choice{i}"] for i in range(NUM_CHOICES)]
        symbols = [x[f"symbol{i}"] for i in range(NUM_CHOICES)]
        return question_color in choice_colors and len(symbols) == len(set(symbols))
    
    def random_letter_counterfactual():
        input_setting = model.sample_input(filter_func=is_input_valid)
        counterfactual = dict(input_setting)  # Make a copy
        
        used_symbols = [input_setting[f"symbol{i}"] for i in range(NUM_CHOICES)]
        alphabet = "ABCDEFGHIJKLMNOPQRSTUVWXYZ"
        available_symbols = [s for s in alphabet if s not in used_symbols]
        new_symbols = random.sample(available_symbols, NUM_CHOICES)
        
        for i in range(NUM_CHOICES):
            counterfactual[f"symbol{i}"] = new_symbols[i]
        
        return {
            "input": input_setting,
            "counterfactual_inputs": [counterfactual]
        }
    
    def random_position_counterfactual():
        input_setting = model.sample_input(filter_func=is_input_valid)
        counterfactual = dict(input_setting)  # Make a copy
        
        answer_position = model.run_forward(input_setting)["answer_pointer"]
        available_positions = [i for i in range(NUM_CHOICES) if i != answer_position]
        new_position = random.choice(available_positions)
        
        correct_color = counterfactual[f"choice{answer_position}"]
        counterfactual[f"choice{answer_position}"] = counterfactual[f"choice{new_position}"]
        counterfactual[f"choice{new_position}"] = correct_color
        
        return {
            "input": input_setting,
            "counterfactual_inputs": [counterfactual]
        }
    
    # Generate small datasets for testing
    datasets = {}
    train_size, test_size = 5, 3
    
    for name, generator in [
        ("random_letter", random_letter_counterfactual),
        ("random_position", random_position_counterfactual)
    ]:
        # Train dataset
        train_data = {"input": [], "counterfactual_inputs": []}
        for _ in range(train_size):
            sample = generator()
            train_data["input"].append(sample["input"])
            train_data["counterfactual_inputs"].append(sample["counterfactual_inputs"])
        
        # Test dataset
        test_data = {"input": [], "counterfactual_inputs": []}
        for _ in range(test_size):
            sample = generator()
            test_data["input"].append(sample["input"])
            test_data["counterfactual_inputs"].append(sample["counterfactual_inputs"])
        
        # Create CounterfactualDataset objects
        datasets[f"{name}_train"] = CounterfactualDataset.from_dict(train_data, id=f"{name}_train")
        datasets[f"{name}_test"] = CounterfactualDataset.from_dict(test_data, id=f"{name}_test")
    
    return datasets


class MockModel:
    """Mock model implementation for testing."""
    def __init__(self):
        self.config = type('MockConfig', (object,), {
            'name_or_path': 'mock_model',
            'num_hidden_layers': 4,
            'hidden_size': 32,
            'n_head': 4
        })
        self.device = "cpu"
        self.dtype = torch.float32
    
    def to(self, device=None, dtype=None):
        if device:
            self.device = device
        if dtype:
            self.dtype = dtype
        return self
    
    def generate(self, **kwargs):
        # Create mock outputs
        batch_size = kwargs.get('input_ids', torch.ones(1, 1)).shape[0]
        max_new = kwargs.get('max_new_tokens', 3)
        
        # Create sequences
        sequences = torch.randint(2, 99, (batch_size, max_new))
        
        # Create scores if needed
        scores = None
        if kwargs.get('output_scores', False):
            scores = [torch.rand(batch_size, 100) for _ in range(max_new)]
        
        if kwargs.get('return_dict_in_generate', False):
            return type('GenerationOutput', (object,), {'sequences': sequences, 'scores': scores})
        return sequences
    
    def prepare_inputs_for_generation(self, input_ids, attention_mask):
        pos = torch.arange(input_ids.shape[1]).unsqueeze(0).expand_as(input_ids)
        return {"position_ids": pos}


class MockTokenizer:
    """Mock tokenizer implementation for testing."""
    def __init__(self):
        self.pad_token = "<pad>"
        self.pad_token_id = 0
        self.eos_token = "</s>"
        self.eos_token_id = 1
        self.padding_side = "right"
    
    def __call__(self, texts, **kwargs):
        # Simple tokenization
        if isinstance(texts, str):
            texts = [texts]
            
        batch = []
        for text in texts:
            # Just use character codes
            tokens = [ord(c) % 100 + 2 for c in text]
            batch.append(tokens)
        
        # Pad if needed
        max_len = kwargs.get('max_length', max(len(seq) for seq in batch))
        batch = [seq + [self.pad_token_id] * (max_len - len(seq)) for seq in batch]
        
        # Create tensors
        input_ids = torch.tensor(batch, dtype=torch.long)
        attention_mask = (input_ids != self.pad_token_id).long()
        
        return {"input_ids": input_ids, "attention_mask": attention_mask}
    
    def decode(self, token_ids, skip_special_tokens=False):
        if isinstance(token_ids, torch.Tensor):
            token_ids = token_ids.tolist()
        return "".join(chr((t - 2) + 97) if t >= 2 else "_" for t in token_ids)
    
    def batch_decode(self, sequences, skip_special_tokens=False):
        return [self.decode(seq, skip_special_tokens) for seq in sequences]
    
    def convert_tokens_to_ids(self, token):
        return 0  # Always return pad token id for simplicity


@pytest.fixture
def mock_tiny_lm(monkeypatch):
    """Create a mock LMPipeline that doesn't try to load from HuggingFace."""
    # Create mock model and tokenizer
    mock_model = MockModel()
    mock_tokenizer = MockTokenizer()
    
    # Patch AutoModelForCausalLM.from_pretrained and AutoTokenizer.from_pretrained
    import transformers
    
    def mock_model_from_pretrained(*args, **kwargs):
        return mock_model
    
    def mock_tokenizer_from_pretrained(*args, **kwargs):
        return mock_tokenizer
    
    # Apply patches
    monkeypatch.setattr(transformers.AutoModelForCausalLM, 'from_pretrained', mock_model_from_pretrained)
    monkeypatch.setattr(transformers.AutoTokenizer, 'from_pretrained', mock_tokenizer_from_pretrained)
    
    # Create pipeline with mocked components
    pipeline = LMPipeline("mock_model", max_new_tokens=3)
    
    return pipeline


@pytest.fixture
def token_positions(mock_tiny_lm, mcqa_causal_model):
    """Create token position identifiers for the MCQA task."""
    # Define a function to get the last token
    def get_last_token(prompt):
        token_ids = mock_tiny_lm.load(prompt)["input_ids"][0]
        return [len(token_ids) - 1]
    
    # Create TokenPosition objects
    return [
        TokenPosition(get_last_token, mock_tiny_lm, id="last_token")
    ]


@pytest.fixture
def model_units_list(mock_tiny_lm, token_positions):
    """Create model units list for testing."""
    units = []
    layers = [0, 2]
    
    for layer in layers:
        for token_position in token_positions:
            unit = ResidualStream(
                layer=layer,
                token_indices=token_position,
                shape=(mock_tiny_lm.model.config.hidden_size,),
                target_output=True
            )
            units.append([unit])
    
    return units


@pytest.fixture
def mock_intervenable_model():
    """Create a mock intervenable model with the necessary methods."""
    class MockIntervenableModel:
        def __init__(self):
            self.model = MagicMock()
            self.interventions = {
                'test_intervention': MagicMock()
            }
        
        def set_device(self, device, set_model=False):
            pass
    
    return MockIntervenableModel()